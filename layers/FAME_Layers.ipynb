{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Parameter\n",
    "\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree, softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAME Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FAME(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, sens_attribute_tensor):\n",
    "        super(FAME, self).__init__(aggr='mean')  \n",
    "        self.lin = nn.Linear(in_channels, out_channels)\n",
    "        self.sensitive_attr = sens_attribute_tensor\n",
    "        self.bias_correction = nn.Parameter(torch.rand(1))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x)\n",
    "    \n",
    "    def message(self, x_j, edge_index, size):\n",
    "        row, col = edge_index\n",
    "        deg = degree(row, size[0], dtype=x_j.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "        \n",
    "        group_difference = self.sensitive_attr[row] - self.sensitive_attr[col]\n",
    "        \n",
    "        fairness_adjustment = (1 + self.bias_correction * group_difference.view(-1, 1))\n",
    "\n",
    "        return fairness_adjustment * norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A_FAME Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A_FAME(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, sens_attribute_tensor):\n",
    "        super(A_FAME, self).__init__(aggr='add') \n",
    "        self.lin = Linear(in_channels, out_channels) \n",
    "        self.att = Linear(2 * out_channels, 1) \n",
    "        \n",
    "        self.sensitive_attr = sens_attribute_tensor \n",
    "        self.bias_correction = Parameter(torch.rand(1))  \n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x)\n",
    "\n",
    "    def message(self, edge_index, x_i, x_j, size_i):\n",
    "        x_cat = torch.cat([x_i, x_j], dim=-1)  \n",
    "        alpha = self.att(x_cat)\n",
    "\n",
    "        row, col = edge_index\n",
    "        group_difference = self.sensitive_attr[row] - self.sensitive_attr[col]\n",
    "\n",
    "        fairness_adjustment = self.bias_correction * group_difference.view(-1, 1)\n",
    "        alpha = alpha + fairness_adjustment\n",
    "\n",
    "        alpha = softmax(alpha, edge_index[0], num_nodes=size_i)\n",
    "\n",
    "        return alpha * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return aggr_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fair_GNN(torch.nn.Module):\n",
    "    def __init__(self, data, layers=1, hidden=128, dropout=0):\n",
    "        super(Fair_GNN, self).__init__()\n",
    "        # sens_attribute_tensor = torch.tensor(data.sensitive_attr.values, dtype=torch.long)\n",
    "        \n",
    "        self.conv1 = FAME(data.num_node_features, hidden, sens_attribute_tensor)\n",
    "        # self.conv1 = A_FAME(data.num_node_features, hidden, sens_attribute_tensor)\n",
    "\n",
    "        self.fc = Linear(hidden, 2)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index, *args, **kwargs):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
